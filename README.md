# ğŸ‡¹ğŸ‡· Turkish Text Classifier

A modern, full-stack web application for Turkish text classification using state-of-the-art transformer models like BERT and ModernBERT.

![Turkish Text Classifier](https://img.shields.io/badge/Language-Turkish-red)
![FastAPI](https://img.shields.io/badge/FastAPI-0.111.0-green)
![React](https://img.shields.io/badge/React-18.0-blue)
![Python](https://img.shields.io/badge/Python-3.10+-blue)

## âœ¨ Features

### ğŸ¯ **Core Functionality**
- **Multi-Model Support**: Easily add any Hugging Face transformer model
- **Single & Batch Prediction**: Classify one text or multiple texts at once
- **Model Comparison**: Compare results from different models side-by-side
- **Real-time Inference**: Fast predictions with lazy model loading

### ğŸ¨ **Modern UI**
- **Dark/Light Mode**: Beautiful themes with smooth transitions
- **Responsive Design**: Works perfectly on desktop and mobile
- **Interactive Results**: Top-K predictions with confidence scores
- **Sample Texts**: Quick testing with pre-built examples
- **Batch Analysis**: Table view with category distribution charts

### ğŸ”§ **Technical Features**
- **FastAPI Backend**: High-performance async API
- **React Frontend**: Modern, component-based UI
- **Docker Support**: Easy deployment and scaling
- **Model Registry**: YAML-based configuration system
- **Metrics & Monitoring**: Built-in performance tracking

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- Node.js 16+
- Your trained models (BERT, ModernBERT, etc.)

### 1. Clone Repository
```bash
git clone https://github.com/yourusername/Turkish-Text-Classifier.git
cd Turkish-Text-Classifier
```

### 2. Setup Backend
```bash
# Install Python dependencies
pip install -r requirements.txt

# Add your models to the models/ directory
# models/
# â”œâ”€â”€ your_model_name/
# â”‚   â”œâ”€â”€ config.json
# â”‚   â”œâ”€â”€ pytorch_model.bin (or model.safetensors)
# â”‚   â”œâ”€â”€ tokenizer.json
# â”‚   â””â”€â”€ ...
```

### 3. Configure Models
Edit `config/models.yaml`:
```yaml
models:
  - name: bert512
    path: ./models/bert512_model
    max_length: 512
    device: auto
  - name: modernbert2048
    path: ./models/modernbert2048_model
    max_length: 2048
    device: auto
```

### 4. Setup Frontend
```bash
cd frontend
npm install
npm run build
cd ..
cp -r frontend/dist/* app/static/
```

### 5. Run Application
```bash
# Using Makefile
make run

# Or directly
uvicorn app.main:app --reload --port 8000
```

Visit: http://localhost:8000

## ğŸ³ Docker Deployment

```bash
# Build image
make docker-build

# Run container
make docker-run

# Or manually
docker build -t turkish-text-classifier .
docker run -p 8000:8000 turkish-text-classifier
```

## ğŸ“Š API Endpoints

### Core Endpoints
- `GET /health` - Health check
- `GET /models` - List available models
- `GET /labels?model=bert512` - Get model labels
- `POST /predict` - Single text prediction
- `POST /predict/batch` - Batch text prediction
- `GET /metrics` - Performance metrics

### Example Usage
```bash
# Single prediction
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "Merkez BankasÄ± faiz kararÄ±nÄ± aÃ§Ä±kladÄ±.", "model": "bert512"}'

# Batch prediction
curl -X POST http://localhost:8000/predict/batch \
  -H "Content-Type: application/json" \
  -d '{"texts": ["Text 1", "Text 2"], "model": "bert512"}'
```

## ğŸ—ï¸ Architecture

```
â”œâ”€â”€ app/                    # FastAPI backend
â”‚   â”œâ”€â”€ main.py            # API endpoints
â”‚   â”œâ”€â”€ config.py          # Configuration
â”‚   â”œâ”€â”€ registry.py        # Model management
â”‚   â”œâ”€â”€ inference.py       # Prediction logic
â”‚   â”œâ”€â”€ schemas.py         # Pydantic models
â”‚   â”œâ”€â”€ metrics.py         # Performance tracking
â”‚   â””â”€â”€ static/            # Built frontend files
â”œâ”€â”€ config/
â”‚   â””â”€â”€ models.yaml        # Model configurations
â”œâ”€â”€ models/                # Your trained models
â”œâ”€â”€ frontend/              # React frontend source
â”œâ”€â”€ tests/                 # Test files
â”œâ”€â”€ Dockerfile            # Container configuration
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ Makefile             # Development commands
```

## ğŸ¯ Adding New Models

1. **Save your model** using `save_pretrained()`:
```python
model.save_pretrained("./models/my_new_model")
tokenizer.save_pretrained("./models/my_new_model")
```

2. **Add to configuration**:
```yaml
models:
  - name: my_new_model
    path: ./models/my_new_model
    max_length: 512
    device: auto
```

3. **Restart the application** - that's it! ğŸ‰

## ğŸ§ª Testing

```bash
# Run tests
make test

# Or with pytest
pytest tests/ -v
```

## ğŸ“ˆ Performance

- **Lazy Loading**: Models loaded only when first used
- **GPU Support**: Automatic CUDA detection and usage
- **Batch Processing**: Efficient multi-text inference
- **Caching**: Smart model and result caching
- **Async API**: Non-blocking request handling

## ğŸ› ï¸ Development

### Frontend Development
```bash
cd frontend
npm run dev  # Development server with hot reload
```

### Backend Development
```bash
uvicorn app.main:app --reload  # Auto-reload on changes
```

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ™ Acknowledgments

- [Hugging Face Transformers](https://huggingface.co/transformers/) for the amazing model library
- [FastAPI](https://fastapi.tiangolo.com/) for the high-performance web framework
- [React](https://reactjs.org/) for the frontend framework

## ğŸ“§ Contact

For questions and support, please open an issue on GitHub.

---

**Made with â¤ï¸ for the Turkish NLP community**
